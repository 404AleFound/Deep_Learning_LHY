# 深度学习基础

## 局部极小值与鞍点

### 临界点及其种类

### 判断临界值种类的方法

使用泰勒级数近似（Tayler series approximation）将损失函数在临界点处展开。
$$
L(\theta)\approx L(\theta')+(\theta-\theta')^Tg+\frac{1}{2}(\theta-\theta')^TH(\theta-\theta')
$$
$g$代表梯度，$H$代表海森矩阵（Hessian matrix）。海森矩阵中存放的是$L$的二次微分，它第$i$行，第$j$列的结果就是把$\theta$的第i个元素对$L(\theta')$做微分，再把$\theta$的第j个元素对$\frac{\partial L(\theta')}{\partial \theta_i}$作微分。

当位于临界点处，$g=0$，因此上式改写为：
$$
L(\theta)\approx L(\theta')+\frac{1}{2}(\theta-\theta')^TH(\theta-\theta')
$$
用向量$v$表示$\theta-\theta'$，故上式又可改写为：
$$
L(\theta)\approx L(\theta')+\frac{1}{2}v^THv
$$

1. 如果对于$v$，$v^THv>0$，这意味着$L(\theta)<L(\theta')$，这是一个局部极小值点。
2. 如果对于$v$，$v^THv<0$，这意味着$L(\theta)>L(\theta')$，这是一个局部极大值点。
3. 如果对于$v$，$v^THv<0$与$v^THv>0$均存在，这是一个鞍点。

算出一个海森矩阵后，不需要把它跟所有的$v$都乘乘看，只要看$H$的特征值。若$H$的所有特征值都是正的，$H$为正定矩阵，则$v^THv > 0$，临界点是局部极小值。若H 的所有特征值都是负的，$H$为负定矩阵，则$v^THv < 0$，临界点是局部极大值。若$H$的特征值有正有负，临界点是鞍点。

### 逃离鞍点的方法

$H$不只可以帮助我们判断是不是在一个鞍点，还指出了参数可以更新的方向。设$\lambda$为$H$的一个特征值$\lambda$，$u$为其对应的特征向量。对于我们的优化问题，可令$u=\theta-\theta'$，则：
$$
u^THu=u^T(\lambda u)=\lambda||u||^2
$$
将此时带入得：
$$
L(\theta)\approx L(\theta')+\lambda||u||^2
$$
若$\lambda<0$，则$\lambda||u||^2<0$，$L(\theta)<L(\theta')$，且$\theta = \theta'+u$，沿着$u$的方向更新$\theta$，损失就会变小。故，在鞍点时，沿着负的特征值对应的特征向量的方向，就可以继续降低损失，模型进行优化。

但是由于海森矩阵需要计算二次微分，计算代价过大，所以几乎没有人用这个方法来逃离鞍点，因为还有一些其他逃离鞍点的方法计算量远小于海森矩阵的方法。

---

*讲到这边会有一个问题：鞍点跟局部极小值谁比较常见？鞍点其实并没有很可怕，如果我们经常遇到的是鞍点，比较少遇到局部极小值，那就太好了。*

*科幻小说《三体III：死神永生》中有这样一个情节：东罗马帝国的国王君士坦丁十一世为对抗土耳其人，找来了具有神秘力量的做狄奥伦娜。狄奥伦娜可以于万军丛中取上将首级，但大家不相信她有这么厉害，想要狄奥伦娜先展示下她的力量。于是狄奥伦娜拿出了一个圣杯，大家看到圣杯大吃一惊，因为这个圣杯本来是放在圣索菲亚大教堂地下室的一个石棺里面，而且石棺是密封的，没有人可以打
开。狄奥伦娜不仅取得了圣杯，还自称在石棺中放了一串葡萄。于是君士坦丁十一世带人撬开了石棺，发现圣杯真的被拿走了，而是棺中真的有一串新鲜的葡萄，为什么迪奥伦娜可以做到这些事呢？是因为狄奥伦娜可以进入四维的空间。从三维的空间来看这个石棺是封闭的，没有任何路可以进去，但从高维的空间来看，这个石棺并不是封闭的，是有路可以进去的。误差表面会不会也一样呢。*

<img src="./assets/image-20250721194534091.png" alt="image-20250721194534091"  />

---

最小值比例（minimum ration），最小值比例代表正特征值数量和总特征值数量的比值。而在研究人员的实践中发现，几乎找不到所有特征值都为正的临界点，也就是说从经验上看，局部极小值并没有那么常见，多数时候损失不再变化仅仅是遇到了鞍点。

结合三体的故事，我的理解是，目前的深度学习往往会设计非常多的参数，多参数意味着模型的多维度，而只要维度足够高那么它总能找到在低维中找不到了路，这也就是为什么局部极小值十分少见。

## 批量和动量

实际上在计算梯度的时候，并不是对所有数据的损失L 计算梯度，而是把所有的数据分成一个一个的批量（batch）

![image-20250721195447363](./assets/image-20250721195447363.png)

### 批量大小对梯度下降算法的影响

### 自适应学习率

### 学习率调度

### 优化总结

### 分类

### 批量归一化



